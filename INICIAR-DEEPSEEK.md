# üöÄ Configuraci√≥n con DeepSeek R1 14B

## 1. Descargar el modelo (solo primera vez)

```powershell
ollama pull deepseek-r1:14b
```

**Nota:** El modelo pesa ~8GB, puede tardar 10-30 minutos dependiendo de tu internet.

## 2. Verificar que est√° descargado

```powershell
ollama list
```

Deber√≠as ver:

```
NAME                ID              SIZE    MODIFIED
deepseek-r1:14b    abc123def456    8.1 GB  X minutes ago
```

## 3. Probar el modelo

```powershell
ollama run deepseek-r1:14b "Clasifica este mensaje como positivo o negativo: El servicio es excelente"
```

Deber√≠a responder algo como:

```
<think>
Analizando el mensaje...
</think>

Clasificaci√≥n: Positivo
El mensaje expresa satisfacci√≥n con el servicio.
```

## 4. Reiniciar el backend

```powershell
docker compose restart backend
```

## 5. Ver logs para confirmar

```powershell
docker logs syntegra-app-backend-1 -f
```

Deber√≠as ver:

```
ü§ñ Usando Ollama: deepseek-r1:14b en http://host.docker.internal:11434/api/generate
```

## 6. Probar subiendo un CSV

Crear archivo `test.csv`:

```csv
text,timestamp
El producto lleg√≥ r√°pido y en perfecto estado,2025-01-05T10:00:00Z
Muy mal servicio, nadie me responde,2025-01-05T11:00:00Z
¬øCu√°nto cuesta el env√≠o?,2025-01-05T12:00:00Z
```

Subirlo en la aplicaci√≥n y esperar ~2-5 minutos (DeepSeek es m√°s lento pero m√°s preciso).

---

## ‚ö° Notas sobre DeepSeek R1 14B

**Ventajas:**

- ‚úÖ Gratis y local (privacidad total)
- ‚úÖ Muy buena precisi√≥n en clasificaci√≥n
- ‚úÖ Razonamiento expl√≠cito (piensa antes de responder)

**Consideraciones:**

- ‚è±Ô∏è M√°s lento que llama3.2 (~5-10 segundos por mensaje)
- üíæ Requiere ~10GB RAM
- üñ•Ô∏è Mejor con GPU NVIDIA (CUDA)

**Recomendaciones:**

- Para lotes grandes (>100 mensajes), dejar procesando y hacer otra cosa
- El modelo "piensa" antes de responder, es normal que tarde
- Si es muy lento, considera cambiar a `deepseek-r1:1.5b` (m√°s r√°pido, menos preciso)

---

## üêõ Soluci√≥n de Problemas

### "Model not found"

```powershell
ollama pull deepseek-r1:14b
```

### "Out of memory"

Cambiar a versi√≥n m√°s peque√±a:

```powershell
ollama pull deepseek-r1:1.5b
```

Y en `.env` cambiar a: `OLLAMA_MODEL=deepseek-r1:1.5b`

### Muy lento

- Verificar que Ollama use GPU: `ollama ps` deber√≠a mostrar uso de GPU
- Si no tienes GPU, considera usar modelo m√°s peque√±o
- O usar OpenAI (m√°s r√°pido pero de pago)

---

**Sistema configurado para DeepSeek R1 14B** üéØ
